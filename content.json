[{"title":"Mac OS高效配置","date":"2018-11-14T11:15:55.000Z","path":"2018/11/14/Mac-OS高效配置/","text":"工具配置用了Mac接近两三年了，期间确实感觉到了Mac对于开发人员的重要性。这里记录下Mac配置的一些有用链接，链接介绍的比较全了，这里就不再照搬了，记录下方便自查。 用了Mac OS一年多的总结 高效Macbook开发之道(工具篇) 以上两个链接是之前配置Mac的时候参考的链接，确实给我使用Mac带来了不少的便利，这里放在这里方便自查。 开发工具jenv install jenv 1$ curl -L -s get.jenv.io | bash 安装成功之后installer会提示你新打开一个窗口或者restart。由于版权问题，jenv无法分发Java SDK，所以我们需要下载Java相应的版本进行安装，安装完成之后需要使用如下命令将Java交由jenv进行管理 12$ mkdir -p $HOME/.jenv/candidates/java$ ln -s /Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/home $HOME/.jenv/candidates/java/1.7.0_45 安装其他的组件： 浏览所有的组件： 1$ jenv all 查看某个组件所有可以获得的版本： 1$ jenv ls maven 安装某个组件的特定版本： 12345$ jenv install maven 3.3.1``` * 列出所有安装的组件：```plain$ jenv ls 更新组件库： 1$ jenv repo update","tags":[{"name":"Mac","slug":"Mac","permalink":"http://yoursite.com/tags/Mac/"}]},{"title":"Git常用命令汇总","date":"2018-11-14T11:12:27.000Z","path":"2018/11/14/Git常用技能/","text":"本文转载自Git常用技能，供自己查阅使用。Git小白教程，阅读廖雪峰的Git教程 工作流Git最核心的一个概念就是工作流。工作区(Workspace)是电脑中实际的目录；暂存区(Index)像个缓存区域，临时保存你的改动；最后是版本库(Repository)，分为本地仓库和远程仓库。 远程仓库添加远程仓库1git remote add origin git@server-name:path/repo-name.git #添加一个远程库 查看远程仓库12git remote #要查看远程库的信息git remote -v #显示更详细的信息 推送分支1git push origin master #推送到远程master分支 抓取分支12345git clone git@server-name:path/repo-name.git #克隆远程仓库到本地(能看到master分支)git checkout -b dev origin/dev #创建远程origin的dev分支到本地，并命名为devgit checkout origin/dev --track #与上面效果一样git pull origin master #从远程分支进行更新 git fetch origin master #获取远程分支上的数据 抓取GitHub上某个pull request到本地12git fetch origin pull/ID/head:BRANCHNAMEgit checkout BRANCHNAME $ git branch –set-upstream branch-name origin/branch-name，可以建立起本地分支和远程分支的关联，之后可以直接git pull从远程抓取分支。 另外，git pull = git fetch + merge to local 删除远程分支1234567$ git push origin --delete bugfixTo https://github.com/lixuefeng123 - [deleted] bugfix# 或者直接push一个空分支$ git push origin :bugfixTo https://github.com/lixuefeng123 - [deleted] bugfix 更新远程分支信息项目往前推进的过程中，远程仓库上经常会增加一些分支、删除一些分支。 所以有时需要与远程同步下分支信息。 1git fetch -p -p就是修剪的意思。它在fetch之后删除掉没有与远程分支对应的本地分支，并且同步一些远程新创建的分支和tag。 历史管理查看历史123git log --pretty=oneline filename #一行显示git log -p -2 #显示最近2次提交内容的差异git show cb926e7 #查看某次修改 版本回退123git reset --hard HEAD^ #回退到上一个版本git reset --hard cb926e7 #回退到具体某个版git reflog #查看命令历史,常用于帮助找回丢失掉的commit 管理修改123git status #查看工作区、暂存区的状态git checkout -- &lt;file&gt; #丢弃工作区上某个文件的修改git reset HEAD &lt;file&gt; #丢弃暂存区上某个文件的修改，重新放回工作区 查看差异1234git diff #查看未暂存的文件更新 git diff --cached #查看已暂存文件的更新 git diff HEAD -- readme.txt #查看工作区和版本库里面最新版本的区别git diff &lt;source_branch&gt; &lt;target_branch&gt; #在合并改动之前，预览两个分支的差异 使用Idea的客户端即可查看不同文件的修改。 删除文件12git rm &lt;file&gt; #直接删除文件git rm --cached &lt;file&gt; #删除文件暂存状态 储藏和恢复1234git stash #储藏当前工作git stash list #查看储藏的工作现场git stash apply #恢复工作现场，stash内容并不删除git stash pop #恢复工作现场，并删除stash内容 分支管理创建分支12git branch develop #只创建分支git checkout -b master develop #创建并切换到 develop 分支 合并分支123git checkout master #切换到主分支git merge --no-ff develop #把 develop 合并到 master 分支，no-ff 选项的作用是保留原分支记录git branch -d develop #删除 develop 分支 标签显示标签12git tag #列出现有标签 git show &lt;tagname&gt; #显示标签信息 创建标签123git tag v0.1 #新建标签，默认位 HEADgit tag v0.1 cb926e7 #对指定的 commit id 打标签git tag -a v0.1 -m &apos;version 0.1 released&apos; #新建带注释标签 操作标签12345git checkout &lt;tagname&gt; #切换到标签git push origin &lt;tagname&gt; #推送分支到源上git push origin --tags #一次性推送全部尚未推送到远程的本地标签git tag -d &lt;tagname&gt; #删除标签git push origin :refs/tags/&lt;tagname&gt; #删除远程标签 Git 设置设置commit的用户和邮箱 1234git config user.name &quot;xx&quot; #设置 commit 的用户git config user.email &quot;xx@xx.com&quot; #设置 commit 的邮箱git commit --amend --author &quot;Li Xuefeng &lt;lixuefeng228503@gmail.com&gt;&quot; #修改上次提交的用户信息git config format.pretty oneline #显示历史记录时，每个提交的信息只显示一行","tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"Hexo使用","date":"2018-11-13T12:07:20.000Z","path":"2018/11/13/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}]},{"title":"Hawkeye:TopN慢query的获取与优化","date":"2018-03-16T07:10:45.000Z","path":"2018/03/16/Hawkeye-TopN慢query的获取与优化/","text":"Hawkeye的底层分析系统基于Blink进行大数据分析，前段时间在优化慢query查询的过程中开发了应用TopN慢query获取的分析任务，其中用到的分析方法适用于其他类似求TopN的问题中。本文的TopN问题属于批处理范畴。 一、背景经过hawkeye之前的慢query分析，每个应用不同程度的存在慢query,大量的慢query大大影响了引擎的性能，也会带来机器资源的浪费。为了优化Ha3的查询性能，我们将分析更深入了一步，分为如下三部分进行慢query的优化。 应用TopN慢query的获取：获取top50的慢query，并分析出查询各阶段耗时占比； 各阶段耗时优化&amp;验证：几种常见的优化手段，这些优化手段均是通过验证有效的方法； 优化收益评估：经过优化带来的价值主要体现在慢查询数的减少和容量预估的增加。 二、TopN慢查询获取流程慢query数据来自应用的访问日志，query数量和应用的访问量有关，通常在千万甚至亿级别。从海量日志中获取TopN慢query属于大数据分析范畴。我们借助Blink的大数据分析能力，采用分治+hash+小顶堆的方式进行获取，即先将query格式进行解析，获取其查询时间，将解析\b后的k-v数据取md5值，然后根据md5值做分片，在每一个分片中计算TopN慢query，最后在所有的TopN中求出最终的TopN。下面图示\b为慢query任务的处理过程。 定时任务每天执行一次，将应用昨天的AccessLog按照上述流程处理一遍，从而获取应用的Top50慢query，从获取的top50慢query可以分析出一些应用访问不合理的地方，虽然不能完全代表高频慢query，但对于Top级的query优化也可以持续的对应用进行优化，提高查询性能，节约机器资源。 三、慢query的优化手段按照平台用户易优化的原则，根据分析的结论数据提供了四种不同的query优化手段，按照阶段分为： 粗排阶段三种： rank_size数过大，建议减少rank_size数量，可能会影响效果，需业务方评估； rank_size数量合理，但粗排阶段耗时占比超过50%，检查海选插件的性能； 正排过滤改为倒排召回，对于特定场景有很大作用，第四部分会讲到具体case； 精排阶段一种： 战马耗时占比超过50%，建议检查战马相关插件的性能。 目前的优化手段在tisplus2平台上进行透出，点击优化可以看到诊断具体慢query的优化建议，如果上述四条建议条件均不满足，则需要联系平台管理员具体\b诊断下慢的原因了。 四、优化的收益第四部分介绍下采用上述优化带来的效果，某应用采用正排过滤改为倒排召回的优化，采用此优化的前提是：1.swift消息无update消息；2.对应字段是可枚举类型；3.非子doc并且seek数远大于match数。采用此优化之后，慢query数直线下降，由百万级别降到0，且容量预估提升了8倍（即相同资源可抗的访问量上涨了8倍）。 五、总结目前慢query的优化功能已上线，用户可以自行查看应用的访问状况和访问较慢的query，并针对性的进行分析和初步的优化，相比较之前是一个从无到有的过程。但是目前的慢query分析还有很多可以优化的地方，比如优化建议的丰富、慢query的自动优化以及实时慢query的分析等，目前的慢query分析已经能起到一定的问题诊断和优化的作用了，未来还需要持续的挖掘和深入。如果你对数据分析和引擎优化方面有心得，欢迎与我交流，也欢迎有才的你加入搜索事业部。","tags":[{"name":"Hawkeye","slug":"Hawkeye","permalink":"http://yoursite.com/tags/Hawkeye/"}]},{"title":"Hawkeye:助力TISPLUS实现数据化运营","date":"2018-03-16T06:57:12.000Z","path":"2018/03/16/Hawkeye-助力TISPLUS实现数据化运营/","text":"背景阅读本文的同时，建议大家先阅读洪震的文章《阿里集团搜索中台TisPlus》。TISPLUS平台的数据分析能力主要由hawkeye提供，但是之前存在如下几个问题：1.数据化场景的功能没有凸显，隐藏较深；2.产品形态设计单一，没有一个较好的产品闭环引导用户关注数据化的结果；3.数据分析内容简单，覆盖面不足，远远达不到让用户数据化运营服务的目标；4.重点关注了数据分析的结果，但缺少衡量数据分析结果为搜索服务本身带来的价值大小。我们深知数据必须和业务、平台本身做深度结合才能产生其核心价值，为了助力TISPLUS平台数据化能力建设，更好的发挥hawjeye的作用，我们项目小组进行了诸如数据化需求调研分析、产品视觉稿设计、hawkeye平台拆分、在TISPLUS平台落地数据化等一系列工作。 hawkeye简介hawkeye是一个数据分析产品，定位于协助维护及优化线上系统，包括但不限于ha3、sp、igraph等引擎，目前主要基于tisplus平台，进行ha3引擎的数据分析以及引擎优化，诸如慢查询、引擎schema合理性、服务稳定性、资源配置合理性的检测及优化等。 系统简介 1.hawkeye平台经过拆分后大体分为三部分 hawkeye-blink：基于Blink完成数据处理的工作，重点是访问日志分析、全量数据分析等，该工程侧重底层的数据分析，借助Blink强大的数据处理能力，每天对于TISPLUS平台所有Ha3应用的访问日志以及全量数据进行分析，工程处理流程图如下： hawkeye-experience：基于hawkeye-blink的分析结果进行更加贴近用户的分析，比如字段信息监测，包括字段类型合理性，字段值单调性监测等，除此之外还包括但不限于kmon无效报警、冒烟case录入情况、引擎降级配置、内存相关配置、推荐行列数配置以及切换时最小服务行比例等检测，hawkeye-experience工程的定位是做一个引擎诊断规则中台，将平时运维人员优化维护引擎的宝贵经验沉淀到系统中来，让每一个新接入的应用可以快速享受这样的宝贵经验，而不是通过一次次的踩坑之后获得，让每位用户拥有一个类似智能诊断专家的角色来优化自己的引擎是我们的目标，也是我们持续奋斗的动力,其中hawkeye-experience的数据处理流程图如下所示： hawkeye-console：web层，提供hawkeye分析结果的各种api以及可视化的监控图表输出。 2.TISPLUS-Ha3平台：搜索引擎服务平台，目前接入了商品中心IC、聚划算、村淘、盒马、评价、优酷、咸鱼、飞猪、lazada等重要业务；3.TISPLUS-sp平台：sp+ha3组成一个完成的引擎服务；4.airflow/matt：搜索调度平台(后更名为matt)，hawkeye有约上百个分析任务，有些是脚本式任务，有些需要上传资源文件，诸如java jar，bash script等，每天有定时触发以及手动触发等多种类型，airflow/matt很好的支持了hawkeye的任务调度；5.容量评估平台：刚上线的应用如何申请资源？有点懵，别上火，容量评估告诉你,容量评估数据的得出，得益于heracles（压测平台）支持大促压测、日常化压测以及容量评估。6.成本平台：数据分析结果为搜索服务带来的价值几何？成本平台衔接数据化运营及其价值，服务优化带来的价值清晰可见；7.sophon：智能化运维平台，目前主要从sophon获取应用关于引擎的一些配置信息，后续将和智能化运维结合，更好的发挥hawkeye在数据化运营这块的作用。 tisplus数据化场景&amp;现阶段成果数据大盘：由原先的混合平铺tag改为单独入口，数据大盘包括内容有：服务总览、Ha3引擎监控指标、Ha3引擎分析指标、成本指标、sp监控指标、资源浪费情况等，是对业务全方位的重点数据展示，通过数据大盘，业务方可以轻松查看业务各个细节数据，时刻做到心中有“数”。其中慢查询相关的分析有慢查询特征分析，慢查询数变化趋势等。 一键诊断：一键诊断的目的是让用户不需要理解系统复杂度的前提下仍然清楚的了解自己引擎的健康状态，做用户的智能诊断专家，因此我们推出健康分这一指标用于衡量引擎健康状态，用户通过健康分可以明确知道自己的服务健康质量如何。同时每一项都给出了诊断时间，配置不合理的简要描述以及详情，优化的收益，并给出子项的健康分以及优化的链接，诊断时间根据数据源的性质分为T+1的检测和实时的检测。 诊断决策：诊断项事先需要定义好相应的规则，规则的获取来自实际的运维经验以及引擎的配置要求等，目前主要包括服务稳定性性检测，资源配置检测以及字段信息检测三大类共计九个小项的检测，将获取的信息经过规则判断，得到诊断分析的结果，同时为相应的诊断项赋予相应的权重和计算子项健康分，每一个诊断项根据其重要程度分为Not_important、important、Very_important三个等级； 计算应用健康分：目前应用健康分计算较为简单有效，仅是将各个子项的健康分以及权重做线性加权得出。 一键诊断的页面触发逻辑详见下图： 目前一键诊断的产品形态如下图所示，当业务处于正常状态时诊断结果如下图所示： 当诊断结果显示应用有需要优化的问题时诊断结果如下图所示： 成本趋势：数据化运营最为关键的点在于如何去衡量数据化带给平台的价值。TISPLUS和成本系统打通之后，任何有价值的优化都得以在成本系统上体现，结合TISPLUS，我们在用户最常运维的页面加入成本趋势，用户可以看到自己服务的累计总成本与累计节省成本，不同时间段的成本对比，与成本相关的操作记录，让每一步的优化与成本下降有“据”可查，同时激励用户持续不断的优化引擎，达到效率和稳定性的一个平衡。 应用日报：平台做了许多数据化的feature，但是数据化的内容，少即是多，我们需要把最精简的，最核心的，最需要用户关心的数据透出，因此我们将这些数据化内容进行分类筛选，选出最核心的数据每天定时发给业务方。我们挑选了诸如成本信息、稳定性信息、诊断信息三部分最核心的信息作为邮件内容发送给业务负责人。 经过上述这些场景的落地，基本形成数据大盘、一键诊断、成本趋势、应用日报这样一个有效的闭环，通过持续的优化线上系统，最终达到效率和稳定性的平衡。如果你对数据分析和引擎优化方面有心得，欢迎与我交流，也欢迎有才的你加入搜索事业部。","tags":[{"name":"Hawkeye","slug":"Hawkeye","permalink":"http://yoursite.com/tags/Hawkeye/"}]},{"title":"jvm 垃圾回收","date":"2017-06-24T09:57:59.000Z","path":"2017/06/24/jvm-垃圾回收/","text":"写这篇文章的目的在于一个读书之后的提炼，读完《深入理解java虚拟机》这本书之后深感需要提炼下关键的知识点，这样针对具体的问题记忆会更加深刻。接下来我将以问题对话的形式记录jvm-垃圾回收的相关知识点。先\b附上\b原文的一个链接link 问：1.地球人都知道，Java有个东西叫垃圾收集器，他让创建的对象不需要像c/cpp那样delete或者free掉，你能不能谈谈,GC是在什么时候，对什么东西，做了什么事情？ 答：首先是什么时候，回答这个问题需要介绍下HotSpot JVM对于堆的划分，它将堆划分为Young区和Old区，Young区\b分为Eden区和survivor1和survivor2三部分。其中GC又分为minor GC（新生代）和full GC（年老代）,其中minor GC发生的条件是Eden区满了的时候，大多数的Java对象具有朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。老年代GC（也叫Full GC），指发生在老年代的GC，当升到老年代的对象大于老年代的剩余空间时触发Full GC,其中GC时间与非GC时间耗时比例超GCTIMERATE的限制会触发OOM。 对什么东西：对从GC root搜索找不到的对象，而且经过第一次标记、清理之后仍然没有复活的对象进行回收。 做了什么事情:能说出诸如新生代做的是复制清理、from survivor、to survivor是干啥用的、老年代做的是标记清理、标记清理后碎片需要整理、复制清理和标记清理有有什么优劣势等。 复制清理：将内存容量划分为大小相等的两块，每次只使用其中一块，然后用完再将存活的对象复制到另一块，将原先的内存块清理掉，实现简单并且运行高效。只是这种算法的代价是将内存缩小为原来的一半，有点高了。v标记清理：复制清理在对象存活率较高时就要进行较多的复制操作，效率将会变低。更关键的是如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有的对象都100%存活的极端情况。所以老年代一般不采用这种算法。标记清理的标记过程和“标记-清除”算法一样，但后续的步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 问：2.Jav\ba和C++在内存分配和管理上有什么区别？ 答：Java与C++之间有一堵由动态内存分配和垃圾收集技术所围成的高墙，墙外面的人想进去，墙里面的人想出来。 对于从事C和C++程序开发的开发人员来说，在内存管理领域，他们既是拥有最高权利的皇帝，也是从事最基础工作的劳动人民—–既拥有每一个对象的所有权，又担负着每一个对象从生命开始到终结的维护责任。 对于Java程序员来说，虚拟机的自动内存分配机制的帮助下，不再需要为每一个new操作去写配对的delete/free代码，而且不容易出现内存泄露和内存溢出问题，看起来由虚拟机管理内存一切都很美好。不过，也正是因为Java程序员把内存控制的权利交给Java虚拟机，一旦出现内存泄露和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那排查错误将会是一项异常艰难的工作。并且好的Java程序在编写的时候肯定要考虑GC的问题，怎样定义static对象，怎样new对象效率更高等等问题，简称面向GC的编程也可以说Java的内存分配管理是一种托管的方式，托管于JVM。C++经过编译时直接编译成机器码，而Java是编译成字节码，由JVM解释执行。C++是编译型语言，而Java兼具编译型和解释型语言的特点。 问：3.java虚拟机规范将JVM虚拟机所管理的内存分为几部分？ 答： 程序计数器：是一块较小的内存空间，它的作用可以看做是当前线程所执行字节码的行号指示器，属于线程私有，和线程的生命周期相同。 Java虚拟机栈：线程私有，生命周期和线程相同。Java虚拟机栈描述的是Java方法（区别于native的本地方法）执行的内存模型，每个方法被执行的时候会同时创建一个栈帧用于存储局部变量表、\b操作栈、动作链接、方法出口等信息。每个方法被调用直到\b执行完成的过程，都对应着一个栈帧在虚拟机栈中入栈和出栈的过程。 本地方法栈：与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法(也就是字节码)服务，而本地方法栈则为虚拟机所使用到的Native方法服务。 方法区：用于存储已被虚拟机加载的类信息、常量、静态变量等，被所有的线程所共享。 java堆：被所有的线程所共享，所有的对象实例以及数组都要在堆上分配，class对象也在这里。 问：4.有哪些方法可以判断一个对象已经可以被回收，JVM怎么判断一个对象已经消亡可以被回收？ 答： 1.引用计数算法：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器就加1；当引用失效时，计数器值就减1；任何时刻计数器都为0的对象就是不可能再被使用的。 Java语言没有选用引用计数法来管理内存，因为引用计数法不能很好的解决循环引用的问题。2.根搜索算法：在主流的商用语言中，都是使用根搜索算法来判定对象是否存活的。GC Root Tracing 算法思路就是通过一系列的名为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连，即从GC Roots到这个对象不可达，则证明此对象是不可用的。 问：5.那些对象可以作为GC Roots？ 答： 虚拟机栈（栈帧中的本地变量表）中的引用的对象 方法区中的类静态属性引用的对象 方法区中的常量引用的对象 本地方法栈中JNI（Native方法）的引用对象 问：6.java代码编译的结果？ 答：是字节码文件.class 问：7.Java中的static变量和static方法在vJVM运行中的内存分配有什么不同？ 答：比较条目 | 静态对象 | 非静态对象—-|——|—-拥有属性： | 是类共同拥有的 | 是类各对象独立拥有的内存分配： | 内存空间上是固定的 | 空间在各个附属类里面分配分配顺序： | 先分配静态对象的空间 | 继而再对非静态对象分配空间，初始化顺序是先静态 问：8.在heap中没有类实例的时候，类信息还存在于JVM吗？ 存在于什么地方？ 答：存在，应该是在方法区，包括类信息和常量等，对象的引用在java虚拟机栈中。","tags":[{"name":"GC","slug":"GC","permalink":"http://yoursite.com/tags/GC/"},{"name":"G1","slug":"G1","permalink":"http://yoursite.com/tags/G1/"}]},{"title":"jvm 类加载","date":"2017-06-24T09:57:45.000Z","path":"2017/06/24/jvm-类加载/","text":"上篇文章讲述了jvm垃圾回收的一些知识点，这篇文章提炼下jvm类加载机制相关的一些知识点。先附上参考link 类的加载过程分析：.java文件——&gt;通过jdk环境相关\b指令编译（javac）——&gt;.class文件——v&gt;JVM初始化之后，如果有类的执行、调用等相关操作，JVM就会将.class文件加载到内存中，并开始下面的一系列处理：（链接-&gt;初始化）。 一、关于classLoader： 首先我们要搞清楚一点，ClassLoader是Java用于加载类的一个机制。等到程序运行时，JVM先初始化，在JVM初始化的过程中，JVM生成几个ClassLoader，JVM调用指定的ClassLoader去加载.class文件等各类路径、文件的类。 程序运行时类的加载实际过程： JDK执行指令去寻找jre目录，寻找jvm.dll，并初始化JVM； 产生一个Bootstrap Loader（启动类加载器）； BootstrapLoader自动加载ExtendedLoader（标准扩展类加载器），并将其父Loader设为Bootstrap Loader； BootstrapLoader自动加载AppClassLoader（系统类加载器），并将其父Loader设为Extended Loader； 最后由AppClassLoader加载HelloWorld类。 各类classLoader的关系图解： 二、整个类的加载流程： 编译 -&gt; 加载 -&gt; 链接（验证+准备+解析）-&gt;初始化（使用前的准备）-&gt;使用-&gt; 卸载 三、类加载过程的原理分析： 四、虚拟机规范严格规定了如下5中种情况必须立即对类进行“初始化”： 1.遇到new、getstatic、putstatic或者invokestatic这四条字节码指令时，如果类没有进行初始化，需要先触发其初始化；2.使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，需要先触发其初始化；3.当初始化一个类的时候，如果\b发现其父类还没有进行过初始化，则需要先触发其父类的初始化；4.当虚拟机启动的时候，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。5.当使用JDK 1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果为REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄对应的类\b没有进行过初始化，则需要先触发其初始化。","tags":[{"name":"类加载体系","slug":"类加载体系","permalink":"http://yoursite.com/tags/类加载体系/"}]},{"title":"【重构 改善既有代码的设计】读书笔记","date":"2017-06-24T02:59:59.000Z","path":"2017/06/24/【重构-改善既有代码的设计】读书笔记/","text":"重构：在代码写好之后改进它的设计。任何一个傻瓜都能写出计算机可以理解的代码。唯有写出人类容易理解的代码，才是优秀的程序员。 重构（名词）：对软件内部结构的一种调整，目的是在不改变软件可观察行为的前提下，提高其可理解性，降低其修改成本。 重构（动词）：使用一系列重构手法，在不改变软件可观察行为的前提下，调整其结构。 因为重点阅读了7-9章，这里将7-9张章的读书笔记总结如下：在对象之间搬移特性在对象的设计过程中，『决定把责任放在哪儿』即使不是最重要的事，也是最重要的事之一。 搬移函数 Move Method你的程序中，有个函数与其所在类之外的另一个类进行更多交流：调用后者，或被后者调用 -&gt; 在该函数最常引用的类中建立一个有类似行为的新函数。将旧函数变成一个单纯的委托函数，或是将旧函数完全移除 检查源类中被源函数所使用的一切特性（包括字段和函数），考虑它们是否也该被搬迁（如果某个特性只被你打算搬移的那个函数用到，就应该将它一并搬迁。如果另有其他函数使用了这个特性，你可以考虑将使用该特性的所有函数全都一并搬迁。有时候，搬移一组函数比逐一搬移简单些） 检查源类的子类和超类，看看是否有该函数的其他声明（如果出现其他声明，你或许无法进行搬移，除非目标类也同样表现出多态性） 在目标类中声明这个函数（你可以选择一个更有意义的名称） 将源函数的代码复制到目标函数中。调整后者，使其能在新类中正常运行（如果源函数包含异常处理，你得判断逻辑上应该由哪个类来处理这一异常。如果应该由源类来负责，就把异常处理留在原地） 编译目标类 决定如何从源函数正确引用目标对象 修改源函数，使之成为一个纯委托函数 编译，测试 决定是否删除源函数，或将它当作一个委托函数保留下来（如果你经常要在源对象中引用目标函数，那么将源函数作为委托函数保留下来会比较简单） 如果要移除源函数，请将源类中对源函数的所有调用，替换为对目标函数的调用 编译，测试 搬移字段 Move Field你的程序中，某个字段被其所在类之外的另一个类更多的用到 -&gt; 在目标类新建一个字段，修改源字段的所有用户，令它们改用新字段 如果字段的访问级是 public，使用 Encapsulate Field 将它封装起来（如果你有可能移动那些频繁访问该字段的函数，或如果有许多函数访问某个字段，先使用 Self Encapsulate Field 也许会有帮助） 编译，测试 在目标类中建立与源字段相v同的字段，并同时建立相应的设值/取值函数 编译目标类 决定如何在源对象中引用目标对象（首先看是否有一个现成的字段或函数可以帮助你得到目标对象，如果没有，就看能否轻易建立这样一个函数。如果还不行，就得在源类中新建一个字段来存放目标对象。这可能是个永久性修改，但你也可以让它是暂时的，因为后续重构可能会把这个新建字段除掉） 删除源字段 将所有对源字段的引用替换为对某个目标函数的调用 编译，测试 提炼类 Extract Class某个类做了应该由两个类做的事 -&gt; 建立一个新类，将相关的字段和函数从旧类搬移到新类 决定如何分解类所负的责任 建立一个新类，用以表现从旧类中分离出来的责任（如果旧类剩下的责任与旧类名称不符，为旧类更名） 建立『从旧类访问新类』的连接关系（有可能需要一个双向链接。但是在真正需要它之前，不要建立『从新类通往旧类』的链接） 对于你想搬移的每一个字段，运用 Move Field 搬移之 每次搬移后，编译、测试 使用 Move Method 将必要函数搬移到新类，先搬移低层函数（也就是『被其他函数调用』多于『调用其他函数』的函数），再搬移较高层函数 每次搬移之后，编译、测试 检查，精简每个类的接口（如果你建立其双向链接，检查是否可以将它改为单向连接） 决定是否公开新类。如果你的确需要公开它，就要决定让它成为引用对象还是不可变的值对象 这里也存在危险性。如果需要确保两个对象同时被锁定，你就面临事务问题，需要使用其他类型的共享锁。 将类内联化 Inline Class某个类没有做太多事情 -&gt; 将这个类的所有特性搬移到另一个类中，然后移除原类 在目标类身上声明源类的 public 协议，并将其中所有函数委托至源类（如果『以一个独立接口表示源类函数』更合适的话，就应该在内联之前先使用 Extract Interface） 修改所有源类引用点，改而引用目标类（将源类声明为 private，以斩断包之外的所有引用可能。同时修改源类的名称，这便可使编译器帮助你捕捉到所有对于源类的隐藏引用点） 编译，测试 运用 Move Method 和 Move Field，将源类的特性全部搬移到目标类 为源类举行一个简单的『丧礼』。 隐藏委托关系 Hide Delegate客户通过一个委托类来调用另一个对象 -&gt; 在服务类上建立客户所需的所有函数，用以隐藏委托关系 对于每一个委托关系中的函数，在服务对象端建立一个简单的委托函数 调整客户，令它只调用服务对象提供的函数（如果使用者和服务提供者不在同一个包） 每次调整后，编译并测试 如果将来不再有任何客户需要用到 Delegate，便课移除服务对象中的相关访问函数 编译，测试 移除中间人 Remove Middle Man某个类做了过多的简单委托动作 -&gt; 让客户直接调用受托类 建立一个函数，用以获得受托对象 对于每个委托函数，在服务类中删除该函数，并让需要调用该函数的客户转为调用受托对象 处理每个委托函数后，编译、测试 引入外加函数 Introduce Foreign Method你需要为提供服务的类增加一个函数，但你无法修改这个类 -&gt; 在客户类中建立一个函数，并以第一参数形式传入一个服务类实例，外加函数终究是权宜之计。如果有可能，你仍然应该将这些函数搬移到它们的理想家园。如果由于代码所有权的原因使你无法这么做，就把外加函数交给服务类的拥有者，请他帮你在服务类中实现这个函数。 在客户类中建立一个函数，用来提供你需要的功能（这个函数不应该调用客户类的任何特性。如果它需要一个值，把该值当做参数传给它） 以服务类实例作为该函数的第一个参数 将该函数注释为『外加函数(foreign method)，应该在服务类实现』（这么一来，如果将来有机会将外加函数搬移到服务类中时，你便可以轻松找出这些外加函数） 引入本地扩展 Introduce Local Extension你需要为服务类提供一些额外函数，但你无法修改这个类 -&gt; 建立一个新类，使它包含这些额外函数。让这个扩展品陈伟源类的子类或包装类 建立一个扩展类，将它作为原始类的子类或包装类 在扩展类中加入转型构造函数（所谓『转型构造函数』是指『接受原对象作为参数』的构造函数。如果采用子类化方案，那么转型构造函数应该调用适当的超类构造函数；如果采用包装类方案，那么转型构造函数应该将它得到的传入参数以实例变量的形式保存起来，用作接受委托的原对象） 在扩展类中加入新特性 根据需要，将原对象替换为扩展对象 将针对原始类定义的所有外加函数搬移到扩展类中 重新组织数据自封装字段 Self Encapsulate Field你直接访问一个字段，但与字段之间的耦合关系逐渐变得笨拙 -&gt; 为这个字段建立取值/设值函数，并且只以这些函数来访问字段 为待封装字段建立取值/设值函数 找出该字段的所有引用点，将它们全部改为调用取值/设值函数 将该字段声明为 private 复查，确保找出所有引用点 编译，测试 以对象取代数据值 Replace Data Value with Object你有一个数据项 -&gt; 需要与其他数据和行为一起使用才有意义 -&gt; 将数据项变成对象 为待替换数值新建一个类，在其中声明一个 final 字段，其类型和源类中的待替换数值类型一样。然后在新类中加入这个字段的取值函数，再加上一个接受此字段为参数的构造函数 编译 将源类中的待替换数值字段的类型改为前面新建的类 修改源类中该字段的取值函数，令它调用新类的取值函数 如果源类构造函数中用到这个待替换字段（多半是赋值动作），我们就修改构造函数，令它改用新类的构造函数来对字段进行赋值动作 修改源类中待替换字段的设值函数，令它为新类创建一个实例 编译，测试 现在，你有可能需要对新类使用 Change Value to Reference 将值对象改为引用对象 Change Value to Reference你从一个类衍生出许多彼此相等的实例，希望将它们替换为同一个对象 -&gt; 将这个值对象编程引用对象 使用 Replace Constructor with Factory Method 编译，测试 决定由什么对象负责提供访问新对象的途径（可能是一个静态字典或一个注册表对象；也可以使用多个对象作为新对象的访问点） 决定这些引用对象应该预先创建号，或是应该动态创建（如果这些引用对象是预先创建号的，而你必须从内存中将它们读取出来，那么就得去报它们在被需要的时候能够被及时加载） 修改工厂函数，令它返回引用对象（如果对象是预先创建号的，你就需要考虑：万一有人请求一个并不存在的对象，要如何处理错误；可能希望对工厂函数使用 Rename Method，使其传达这样的信息：它返回的是一个已存在的对象） 编译，测试 将引用对象改为值对象 Change Reference to Value你有一个引用对象，很小且不可变，而且不易管理 -&gt; 将它变成一个值对象 检查重构目标是否为不可变对象，或是否可修改为不可变对象（如果该对象目前还不是不可变的，就使用 Removing Setting Method，直到它成为不可变的为止；如果无法将该对象修改为不可变的，就放弃使用本项重构） 建立 equals() 和 hashCode() 编译，测试 考虑是否可以删除工厂函数，并将构造函数声明为 public 以对象取代数组 Replace Array with Object你有一个数组，其中的元素各自代表不同的东西 -&gt; 以对象替换数组。对于数组中的每个元素，以一个字段来表示 新建一个类表示数组所拥有的信息，并在其中以一个 public 字段保存原先的数组 修改数组的所有用户，让它们改用新类的实例 编译，测试 逐一为数组元素添加取值/设值函数。根据元素的用途，为这些访问函数命名。修改客户代码，让它们动过访问函数取用数组内的元素。每次修改后，编译并测试 当所有对数组的直接访问都转而调用访问函数后，将新类中保存该数组的字段声明为 private 编译 对于数组内的每一个元素，在新类中创建一个类型相当的字段。修改该元素的访问函数，令它改用上述的新建字段 每修改一个元素，编译并测试 数组的所有元素都有了相应字段之后，删除该数组 复制『被监视的数据』 Duplicate Observed Data你有一些领域数据置身于 GUI 控件中，而领域函数需要访问这些数据 -&gt; 将该数据复制到一个领域对象中。建立一个 Observer 模式，用以同步领域对象和 GUI 对象内的重复数据 修改展现类，使其成为领域类的 Observer[GOF]（如果尚未有领域类，就建立一个；如果没有『从展现类到领域类』的关联，就将领域类保存与展现类的一个字段中） 针对 GUI 类中的领域数据，使用 Self Encapsulate Field 编译，测试 在事件处理函数中调用设值函数，直接更新 GUI 组件 编译，测试 在领域类中定义数据及其相关访问函数（确保领域类中的设值函数能够触发 Observer 模式的通报机制；对于被观察的数据，在领域类中使用与展现类所用的相同类型来保存。后续重构中你可以自由改变这个数据类型） 修改展现类中的访问函数，将它们的操作对象改为领域对象（而非 GUI 组件） 修改 Observer 的 update()，使其从相应的领域对象中将所需数据复制给 GUI 组件 编译，测试 将单向关联改为双向关联 Change Unidirectional Association to Bidirectional两个类都需要使用对方特性，但期间只有一条单向连接 -&gt; 添加一个反向指针，并使修改函数能够同时更新两条连接 \b在被引用类中增加一个字段，用以保存反向指针 决定由哪个类——引用端还是被引用端——控制关联关系 在被控端建立一个辅助函数，其命名应该清楚指出它的有限用途 如果既有的修改函数在控制端，让它负责更新方向指针 如果既有的修改函数在被控端，就在控制端建立一个控制函数，并让既有的修改函数调用这个新建的控制函数 将双向关联改为单向关联 Change Bidirectional Association to Unidirectional两个类之间有双向关联，但其中一个类如今不再需要另一个类的特性 -&gt; 去除不必要的关联 双向关联很有用，但是也必须为它付出代价，那就是维护双向连接、确保对象被正确创建和删除增加的复杂度。而且，由于很多程序员并不习惯使用双向关联，它往往成为错误之源。大量的双向连接也很容易造成『僵尸对象』：某个对象本来已经该死亡了，却仍然保留在系统中，因为对它的引用还没有完全清除。 找出保存『你想去除的指针』的字段，检查它的每一个用户，判断是否可以去除该指针（不但要检查直接访问点，也要检查调用这些直接访问点的函数） 如果客户使用了取值函数，先运用 Self Encapsulate Field 将待删除字段自我封装起来，然后使用 Substitute Algorithm 对付取值函数，令它不再使用该字段。然后编译、测试 如果客户并未使用取值函数，那就直接修改待删除字段的所有被引用点：改以其他途径获得该字段所保存的对象。每次修改后，编译并测试 如果已经没有任何函数使用待删除字段，移除所有对该字段的更新逻辑，然后移除该字段 编译，测试 以字面常量取代魔法数 Replace Magic Number with Symbolic Constant你有一个字面数值，带有特别含义 -&gt; 创造一个常量，根据其意义为它命名，并将上述的字面数值替换为这个常量 声明一个常量，令其值为原本的魔法数值 找出这个魔法数的所有引用点 检查是否可以使用这个新声明的常量来替换该魔法数。如果可以，便以此常量替换之 编译 所有魔法数都被替换完毕后，编译并测试。此时整个程序应该运转如常 封装字段 Encapsulate Field 你的类中存在一个 public 字段 -&gt; 将它声明为 private，并提供相应的访问函数，为 Public 字段提供取值/设值函数 找到这个类以外使用该字段的所有地点。用取值/设置函数进行替代 每次修改之后，编译并测试 将字段的所有用户修改完毕后，把字段声明为 private 编译，测试 封装集合 Encapsulate Collection有个函数返回一个集合 -&gt; 让这个函数返回该集合的一个只读副本，并在这个类中提供添加/移除集合元素的函数（类似 MVC 的 M） 加入为集合添加/移除元素的函数 将保存集合的字段初始化为一个空集合 编译 找出集合设值函数的所有调用者。你可以修改那个设值函数，让它使用上述新建立的『添加/移除元素』函数；也可以直接修改调用端，改让它们调用上述新建立的『添加/移除元素』函数 编译，测试 找出所有『通过取值函数获得集合并修改其内容』的函数。逐一修改这些函数，让它们改用添加/移除函数。每次修改后，编译并测试 修改完上述所有『通过取值函数获得集合并修改集合内容』的函数后，修改取值函数自身，使它返回该集合的一个只读副本 编译，测试 找出取值函数的所有用户，从中找出应该存在于集合所属对象内的代码。运用 Extract Method 和 Move Method 将这些代码移到宿主对象去 修改现有取值函数的名字，然后添加一个新取值函数，使其返回一个枚举。找出旧取值函数的所有被实用点，将它们都改为使用新取值函数 如果这一步跨度太大，可以先使用 Rename Method 修改原取值函数的名称；再建立一个新取值函数用以返回枚举；最后再修改所有调用者，使其调用新取值函数 编译，测试 以数据类取代记录 Replace Record with Data Class你需要面对传统编程环境中的记录结构 -&gt; 为该记录创建一个『哑』数据对象 新建一个类，表示这个记录 对于记录中的每一项数据，在新建的类中建立对应的一个 private 字段，并提供相应的取值/设值函数 以类取代类型码 Replace Type Code with Class 为类型码建立一个类 修改源类的实现，让它使用上述新建的类 编译，测试 对于源类中每一个使用类型码的函数，相应建立一个函数，让新函数使用新建的类 逐一修改源类用户，让它们使用新接口 每修改一个用户，编译并测试 删除使用类型码的旧接口，并删除保存旧类型码的静态变量 编译，测试 以子类取代类型码 Replace Type Code with Subclasses你有一个不可变的类型码，它会影响类的行为 -&gt; 以子类取代这个类型码 使用 Self Encapsulate Field 将类型码自我封装起来（如果类型码被传递给构造函数，就需要将构造函数换成工厂函数） 为类型码的每一个数值建立一个相应的子类。在每个子类中覆写类型码的取值函数，使其返回相应的类型码值 每建立一个新的子类，编译并测试 从超类中删掉保存类型码的字段。将类型码访问函数声明为抽象函数 编译，测试 以 State/Strategy 取代类型码 Replace Type Code with State/Strategy你有一个类型码，它会影响类的行为，但你无法通过继承手法消除它 使用 Self Encapsulate Field 将类型码自我封装起来 新建一个类，根据类型码的用途为它命名。这就是一个状态对象 为这个新类添加子类，每个子类对应一种类型码 在超类中建立一个抽象的查询函数，用以返回类型码。在每个子类中覆写该函数，返回确切的类型码 编译 在源类中建立一个字段，用以保存新建的状态对象 调整源类中负责查询类型码的函数，将查询动作转发给状态对象 调整源类中为类型码设值的函数，将一个恰当的状态对象子类赋值给『保存状态对象』的那个字段 编译，测试 以字段取代子类 Replace Subclass with Fields你的各个子类的唯一差别只在『返回常量数据』的函数身上 -&gt; 修改这些函数，使它们返回超类中的某个（新增）字段，然后销毁子类 对所有子类使用 Replace Constructor with Factory Method 如果有任何代码直接引用子类，令它改而引用超类 针对每个常量函数，在超类中声明一个 final 字段 为超类声明一个 protected 构造函数，用以初始化这些新增字段 新建或修改子类构造函数，使他调用超类的新增构造函数 编译，测试 在超类中实现所有的常量函数，令它们返回相应字段值，然后将该函数从子类中删掉 每删除一个常量函数，编译并测试 子类中所有的常量函数都被删除后，使用 Inline Method 将子类构造函数内联到超类的工厂函数中 编译，测试 将子类删掉 编译，测试 重复『内联构造函数、删除子类』过程，直到所有子类都被删除 简化条件表达式相比于面向过程程序，免息那个对象程序的条件表达式通常比较少，这是因为很多条件行为都被多态机制处理掉了。多态还有一种十分有用但鲜为人知的用途：通过 Introduce Null Object 去除对于 null 值的检验。 分解条件表达式 Decompose Conditional你有一个复杂的条件(if-then-else)语句 -&gt; 从 if, then, else 三个段落中分别提炼出独立函数 将 if 段落提炼出来，构成一个独立函数 将 then 段落和 else 段落都提炼出来，各自构成一个独立函数 合并条件表达式 Consolidate Conditional Expression你有一系列条件测试，都得到相同结果 -&gt; 将这些测试合并为一个条件表达式，并将这个条件表达式提炼成为一个独立函数 确定这些条件语句都没有副作用（如果条件表达式有副作用，你就不能使用本项重构） 使用适当的逻辑操作符，将一系列相关条件表达式合并为一个 编译，测试 对合并后的表达式实施 Extract Method 合并重复的条件片段 Consolidate Duplicate Conditional Fragments在条件表达式的每个分支上有着相同的一段代码 -&gt; 将折断代码搬移到条件表达式之外 鉴别出『执行方式不随条件变化而变化』的代码 如果这些共通代码位于条件表达式起始处，就将它移到条件表达式之前 如果这些共通代码位于条件表达式尾端，就将它移到条件表达式之后 如果这些共同代码位于条件表达式中段，就需要观察来向前或向后移动 如果共通代码不止一条语句，应该先使用 Extract Method 将共通代码提炼到一个独立函数中，再以前面所说的办法来处理 移除控制标记 Remove Control Flag在一系列布尔表达式中，某个变量带有『控制标记(control flag)』的作用 -&gt; 以 break 语句或 return 语句取代控制标记 找出让你跳出这段逻辑的控制标记值 找出对标记变量赋值的语句，代以恰当的 break 语句或 continue 语句 每次替换后，编译并测试 在未能提供 break 和 continue 语句的编程语言中，可以使用下述办法 运用 Extract Method，将整段逻辑提炼到一个独立函数中 找出让你跳出这段逻辑的控制标记值 找出对标记变量赋值的语句，代以恰当的 return 语句 每次替换后，编译并测试 以 Guard 语句取代嵌套条件表达式 Replace Nested Conditional with Guard Clauses函数中的条件逻辑使人难以看清正常的执行路径 -&gt; 使用 Guard 语句表现所有特殊情况 对于每个检查，放进一个 Guard 语句（要么从函数中返回，要么抛出一个异常） 每次将条件检查替换成 Guard 语句后，编译并测试（如果所有 Guard 语句都导致相同结果，请使用 Consolidate Conditional Expressions） 以多态取代条件表达式 Replace Conditional with Polymorphism你手上有个条件表达式，它根据对象类型的不同而选择不同的行为 -&gt; 将这个条件表达式的每个分支放进一个子类内的覆写函数中，然后将原始函数声明为抽象函数 如果要处理的表达式是一个更大函数中的一部分，首先对条件表达式进行分析，然后使用 Extract Method 将它提炼到一个独立函数去 如果有必要，使用 Move Method 将条件表达式放置到继承结构的顶端 任选一个子类，在其中建立一个函数，使之覆写超类中容纳条件表达式的那个函数。将与该子类相关的条件表达式分支复制到新建函数中，并对它进行适当调整 编译，测试 在超类中删掉条件表达式内被复制了的分支 编译，测试 针对条件表达式的每个分支，重复上述过程，直到所有分支都被移到子类内的函数为止 将超类之中容纳条件表达式的函数声明为抽象函数 引入 Null 对象你需要再三检查某对象是否为 null -&gt; 将 null 值替换为 null 对象 为源类建立一个子类，使其行为就像是源类的 null 版本。在源类和 null 子类中都加上 isNull() 函数，前者的 isNull() 应该返回 false，后者的返回 true 编译 找出所有『请求源对象却获得一个 null』 的地方，修改这些地方，使它们改而获得一个空对象 找出所有『将源对象与 null 做比较的地方』，修改这些地方，使它们调用 isNull() 函数 编译，测试 找出这样的程序点：如果对象不是 null，做 A 动作，否则做 B 动作 对于每一个上述地点，在 null 类中覆写 A 动作，使其行为和 B 动作相同 使用上述被覆写的动作，然后删除『对象是否等于 null』 的条件测试。编译并测试 引入断言 Introduce Assertion某一段代码需要对程序状态做出某种假设 -&gt; 以断言明确表现出这种假设 如果你发现代码假设某个条件始终为真，就加入一个断言明确说明这种情况","tags":[{"name":"重构","slug":"重构","permalink":"http://yoursite.com/tags/重构/"}]},{"title":"【构建高性能Web站点】读书笔记","date":"2017-06-24T02:56:18.000Z","path":"2017/06/24/构建高性能Web站点读书笔记/","text":"高性能Web站点：改善性能和扩展规模的具体做法 一、绪论二、数据的网络传输三、服务器并发处理能力四、动态内容缓存五、动态脚本加速六、浏览器缓存七、Web服务器缓存八、反向代理缓存九、Web组件分离十、分布式缓存v十一、数据库性能优化十二、Web负载均衡十三、共享文件系统十四、内容分发和同步十五、分布式文件系统v十六、数据库扩展当我们对于Web计算和存储都进行了不同程度的扩展后，站点的规模不断的膨胀，这给数据库带来了巨大的查询压力，这时候就需要进行扩展了。在解决性能问题的同时，数据库的扩展还带来一些其他的作用，比如增加存储空间、提高可用性等。下面主要介绍几种常用的数据库扩展思路，它们基本上覆盖了大多数的扩展方式。 1.复制和分离： 主从复制，以Mysql为例，它支持主从复制，配置并不复杂，简单来说主要做到以下两点： 开启主服务器上的二进制日志（log-bin）. 在主服务器和从服务器上分别进行简单的配置和授权。 Mysql的主从复制是依据主服务器的二进制日志进行的，也就是说主服务器日志中记录的操作会在从服务器上进行重放，从而实现复制，所以主服务器必须开启二进制日志，它会自动记录所有对数据库产生更新的操作，也包括潜在的更新操作，比如没有删除任何实际记录的DELETE操作。 读写分离：对于所有的更新操作必须让它作用在主服务器上，这样才能保证所有数据库服务器上的数据一致。这也是任何使用单向复制机制的系统必须遵循的更新原则。这样做不仅保证了多台数据服务器的数据一致性，更重要的是它符合情理，一般而言，大多数站点的数据库读操作要比写操作更加密集。 读写分离带来的问题：我们必须在应用程序中配置多个数据库连接对象，并且修改\b数据访问层的代码，的确应用程序应该知道如何区分读写操作，然而对于将读操作分散到多台从服务器上，应用程序就不那么擅长了，简单的随机分配可能造成多台从服务器的工作量不均衡，更重要的是，当某台服务器发生故障时，应用程序并不知道。当然你也可以不修改应用程序，将这部分工作交给数据库反向代理。Mysql Proxy对多个从服务器实现负载均衡以及可用性监测，这些工作由它来做的确非常合适。 2.垂直分区： 背景：对于数据库写操作频繁的站点来说仅仅采用主从复制和读写分离可能效果并不是那么明显，假如你的主服务器花费了80%的时间在写数据，那么所有的从服务器也将花费更多的时间来同步数据，可想而知，所有的从服务器只能依靠剩余的不到20%的时间来处理你的select请求，这时候，增加从服务器所获得的回报将越来越少，呈现边际效益递减。 \b分离方法：将不同的数据库分布到不同的服务器上，你会发现很多的数据库之间并不存在关系，或者不需要进行（JOIN）查询，我们将如用户的博客数据和好友数据库分别转移到独立的数据库服务器上，这种方式称为垂直分区。 3.水平分区： 当对于数据库表也达到写操作极限的时候就该换一种思路进行水平分区了。 水平分区（Sharding）意味着我们可以将同一数据表中的记录通过特定的算法进行分离，分别保存在不同的数据表中，从而可以部署在不同的数据库服务器上。 一般会使用分区的主键或者外键，主键的话不能是auto_increment类型的，采用一致性Hash算法进行数据的分区，这是一种水平的分区。再加入反向代理处理负载均衡以及可用性检测的工作等。 十七、分布式计算十八、性能监控","tags":[{"name":"Web","slug":"Web","permalink":"http://yoursite.com/tags/Web/"},{"name":"高性能","slug":"高性能","permalink":"http://yoursite.com/tags/高性能/"}]},{"title":"搜索——ElasticSearch的学习笔记","date":"2017-03-16T03:59:31.000Z","path":"2017/03/16/搜索之es的学习/","text":"一、ElasticSearch使用简介最近在学习ElasticSearch的相关知识点，已经完整的阅读了ElasticSearch权威指南这本书，为了学有所获，现在将自己的学习笔记整理如下，仅供大家参考。我将从如下几个方面来向大家介绍。 ElasticSearch 5.2.2的安装使用； ElasticSearch 5.2.2的基础概念； ElasticSearch 5.2.2的简单查询； ElasticSearch 5.2.2的配置相关； ElasticSearch 5.2.2的基本操作。 ElasticSearch是一个实时分布式搜索和分析引擎。它让你以前所未有的速度处理大数据成为可能。它用于全文搜索、结构化搜索、分析以及将这三者混合使用。ElasticSearch是一个基于Apache Lucene（TM）的开源搜索引擎。无论是在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好、功能最全的搜索引擎库。ElasticSearch也是用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful Api来隐藏Lucene的复杂性，从而让全文搜索变得简单。 二、ElasticSearch 5.2.2的安装使用： 安装ElasticSearch 安装ElasticSearch唯一的要求是安装官方新版的Java，地址：www.java.com;你可以从 elasticsearch.org/download 下载最新版本的ElasticSearch。 123curl -L -O http://download.elasticsearch.org/PATH/TO/VERSION.zip &lt;1&gt; unzip elasticsearch-$VERSION.zipcd elasticsearch-$VERSION 运行ElasticSearch ElasticSearch已经准备就绪，执行以下命令可在前台启动： 1./bin/elasticsearch 如果想在后台以守护进程模式运行，添加-d参数。 打开另一个终端进行测试： 1curl &apos;http://localhost:9200/?pretty&apos; 你能看到如下返回信息： 当你看到上图中的信息之后，说明你的ElasticSearch集群已经启动并且正常运行，接下来我们可以开始各种实验了。 三、ElasticSearch 5.2.2的基础概念：本节是ElasticSearch5.2.2的基础概念部分的介绍，主要介绍集群，节点，索引，类型，文档，分片，副本等基础概念。 基本概念：简单的介绍下，详细的见Basic Concepts 集群（cluster）：集群由一个或者多个节点组成，由名称唯一标识； 节点（node）：一个单独的ElasticSearch实例； 索引（index）：文档的集合； 类型（type）：索引的逻辑分类/分区； 文档（document）：能够被索引的信息基础单元； 分片（shard）：索引的物理分区，是一个最小的Lucene索引单元。分为primary shard（主分片）和replica shard（简称replicas）。 副本/备份（replicas）:主分片的备份。 类比关系数据库其中索引、类型、文档的概念可以类比关系型数据库 ElasticSearch 关系型数据库 索引（index） 数据库（database） 类型（type） 表（table） 文档（document） 行记录（row） 字段（field） 列（column） 四、ElasticSearch 5.2.2的简单查询：请求体查询简单查询语句（lite）是一种有效的命令行adhoc查询。但是，如果你想要善用搜索，你必须使用请求体查询（request body search）API。之所以这么称呼，是因为大多数的参数以Json格式所容纳而非查询字符串。本节主要介绍如何使用Elasticsearch进行简单的查询，查询方式分为如下两种方式。 通过REST request URI发送查询参数； 通过REST request body发送查询参数。 首先看一个空查询。 12GET /_search&#123;&#125; 这是一个空查询语句，空查询将会返回索引中所有的文档，同字符串查询一样，你可以查询一个，多个或_all索引（indices）或类型（types）： 12345GET /_search&#123; &quot;from&quot;:30, &quot;size&quot;:10&#125; 因为携带交互数据的GET请求并不被广泛支持，所以search API同样支持POST请求，需要注意一点，Elasticsearch和sql的区别，Es一旦获得了结果，Es就彻底完成了请求，且不保存任何服务端资源或者状态信息，这和SQL里一些得到结果子集再通过状态信息（如游标）得到剩下结果集的情况不同。 Query DSL结构化查询是一种灵活的，多表现形式的查询语言。Elasticsearch在一个简单的JSON接口中用结构化查询来展现Lucene绝大多数的能力。它使得你的查询更加灵活，精准，易于阅读并且易于debug。使用结构化查询，你需要传递query参数：1234GET /_search&#123; &quot;query&quot;: YOUR_QUERY_HERE&#125; 空查询 -{} - 功能上等同于使用match_all查询子句，正如其名字一样，匹配所有的文档： 123456GET /_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125; 更多的例子，如must，should，bool query等等。 过滤器查询条件与文档的相关性用_score这个数值字段来表示，数值越高代表越相关。而有时我们不需要去计算相关性，只需要确定文档满不满足查询条件即可，这时可用filter（过滤器） 1234567891011121314151617# uses a bool query to return all accounts with balances between 20000 and 30000GET /bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;balance&quot;: &#123; &quot;gte&quot;: 20000, &quot;lte&quot;: 30000 &#125; &#125; &#125; &#125; &#125;&#125; 在网上的这个例子中，返回balance在20000~30000的文档，所有满足条件的文档的匹配程度都是”等价”的，没有谁更相关，所以计算分数是毫无意义的。 聚合aggregation（聚合）能够对数据分组和提取统计信息，大致类似SQL中的group by和聚合函数。Elasticsearch能同时分别返回查询结果和聚合结果，从而避免多次查询。 12345678910111213# 类似SQL中的 SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESCGET /bank/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_state&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state.keyword&quot; &#125; &#125; &#125;&#125; 五、ElasticSearch 5.2.2的配置相关：为什么有 Bootstrap ChecksBootstrap Checks 是 Elasticsearch 5.0 新加入的，在之前的 2.x 版本是没有的。之前的版本中，错误的配置会被当成 warning 记录到日志中，但这些信息往往被用户忽视。为了保证一些重要的配置得到应有的重视，Elasticsearch 会在启动时进行 Bootstrap Checks . Bootstrap Checks 会检查很多 Elasticsearch 和系统的配置。在开发模式下，所有没通过的检查都会报 warnings 并写进日志文件，即使检查没通过，依然可以启动节点运行 Elasticsearch；而在生产模式下，任何没通过的 Bootstrap Checks 都会报异常并阻止 Elasticsearch 启动。 开发模式 vs 生产模式Elasticsearch 的 HTTP 默认绑定到localhost，并且 transport 使用内部通信，适用于日常开发；而组成集群时，由于每个 ES 实例要可达，故 transport 必须绑定到外部接口。 一般 Elasticsearch 默认你是在开发模式下工作；一旦配置了诸如network.host的网络配置项，Elasticsearch会认为你处于生产环境。这是避免服务器因不良配置造成数据丢失的重要安全措施。 另外，HTTP 和 transport 可以分别通过 http.host 和 transport.host进行配置，所以配置单点实例可达时，可以用 HTTP 进行测试而无需触发生产模式。 Bootstrap Checks有很多检查项，以 Heap size check为例子，由于 Elasticsearch 是使用 Java 写的，程序在 JVM 上运行，而 JVM 的堆大小是可以配置的。如果 JVM 的起始堆大小不等于最大堆大小，那么在堆 resize 的时候很容易造成系统停滞，为了避免这种resize pauses,一开始就应将两者设置成相等。 类似的检查还有很多，大部分是针对 JVM 配置项的检查，有些检查项只在 Linux 系统上会检查，有些在所有平台都会检查。这里只列举出检查项，不作进一步说明了。 Heap size check File descriptor check Memory lock check Maximum number of threads check Maximum size virtual memory check Maximum map count check Client JVM check Use serial collector check OnError and OnOutOfMemoryError checks 重要的系统配置从上节可知，很多Bootstrap Checks涉及到系统配置，我们需要对系统进行一些配置来使 Elasticsearch 可以获取更多的资源。 一般必须配置以下几条设置： Set JVM heap size Disable swapping Increase sufficient virtual memory Ensure sufficient threads 在哪里配置系统设置取决于你使用的安装包以及你使用的操作系统。JVM配置参数建议通过 jvm.options 配置文件进行配置，当然，也可以通过ES_JAVA_OPTS环境变量来配置。 六、ElasticSearch 5.2.2的基本操作：基础指令下面是在Console 中输入的一些命令，可以依次运行看看结果。 1234567891011121314151617181920212223# Cluster HealthGET _cat/health?v# list of nodesGET /_cat/nodes?v# List All IndicesGET /_cat/indices?v# Create an IndexPUT /customer?prettyGET /_cat/indices?v# Index and Query a DocumentPUT /customer/external/1?pretty&#123; &quot;name&quot;:&quot;John Doe&quot;&#125;GET /customer/external/1?prettyDELETE /customer/external/1# Delete an Index DELETE /customer?pretty 可以看出Elasticsearch的REST API基本格式： 1&lt;REST Verb&gt; /&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt; 索引/替换文档1234567891011# Indexing/Replacing DocumentsPUT /customer/external/1?pretty&#123; &quot;name&quot;: &quot;John Doe&quot;&#125;# using the POST verb instead of PUT since we didn’t specify an IDPOST /customer/external?pretty&#123; &quot;name&quot;: &quot;Jane Doe&quot;&#125; 使用PUT方法需要明确指定ID，两次PUT的id相同则是替换之前的文档，第二次id不同则是创建新的文档 没明确指定ID则使用POST方法 更新文档12345678910111213141516# Updating DocumentsPOST /customer/external/1/_update?pretty&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot; &#125;&#125;POST /customer/external/1/_update?pretty&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot;, &quot;age&quot;: 20 &#125;&#125;# uses a script to increment the age by 5POST /customer/external/1/_update?pretty&#123; &quot;script&quot; : &quot;ctx._source.age += 5&quot;&#125; 更新文档其实就是先删除再新建一个新的文档 1Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot 删除文档12# Deleting DocumentsDELETE /customer/external/2?pretty 直接删除整个index要比删除index里的所有文档更有效率 1It is worth noting that it is much more efficient to delete a whole index instead of deleting all documents with the Delete By Query API. 批处理123456789101112# Batch Processing POST /customer/external/_bulk?pretty&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;John Doe&quot; &#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;Jane Doe&quot; &#125;POST /customer/external/_bulk?pretty&#123;&quot;update&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;doc&quot;: &#123; &quot;name&quot;: &quot;John Doe becomes Jane Doe&quot; &#125; &#125;&#123;&quot;delete&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125; 删除操作后面没有相应的文档数据，只提供要删除的 ID 即可； 批处理对每个操作(action)按顺序依次执行(sequentially and in order)，如果单个操作出错，也会继续执行剩下的操作； 批处理返回结果时，按照请求顺序为每个操作提供一个状态以便用户检查。","tags":[{"name":"ES","slug":"ES","permalink":"http://yoursite.com/tags/ES/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"Sun的jdk命令行工具","date":"2017-03-16T03:58:43.000Z","path":"2017/03/16/jvm的jdk命令行工具/","text":"名称 主要作用 jps JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程 jstat JVM Statistics Monitoring Tool,用于收集Hotspot虚拟机各方面的运行数据 jinfo Configuration Info for Java,显示虚拟机配置信息 jmap Memory Map for Java,生成虚拟机的内存转储快照（heapdump文件） jhat JVM Heap Dump Browser，用于分析heapdump文件，它会建立一个HTTP/HTML服务器，让用户可以在浏览器上查看分析结果 jstack Stack Trace for Java,显示虚拟机的线程快照","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Jvm","slug":"Jvm","permalink":"http://yoursite.com/tags/Jvm/"}]},{"title":"java多线程体系","date":"2017-03-16T03:54:57.000Z","path":"2017/03/16/java多线程体系/","text":"下一本书开始阅读完java并发编程实战，记录一些学习笔记如下作为自己的读书笔记进行记录。 第二章、基础知识2.1.线程安全性： 当多个线程访问某个类的时候，这个类始终都能表现出正确的行为，那么就称这个类是线程安全的。线程安全的\b核心在于正确性。 2.2.原子性： 竞态条件和复合操作， 常见的一种竞态条件类型是“Check-Then-Act”，先检查后执行，即通过一个可能失效的观测结果来决定下一步的动作\b。 复合操作是指“先检查后操作”以及“读取-修改-写入”这种操作。 2.3.加锁机制：内置锁和重入， 内置锁即synchronized block，包括两部分：一个作为锁的对象引用，一个作为由这个锁保护的代码块。 重入：如果某个线程试图获得一个已经由它持有的锁，那么这个请求就会成功。 2.4.用锁来保护状态： 复合操作需要额外的加锁机制。 2.5.活跃性和性能： 活跃性问题的形式之一就是无意之中造成的无限循环问题。\b* 性能相关：当执行时间较长的计算或者可能无法快速完成的操作时，例如：网络IO或者控制台IO时，一定不要持有锁。 第三章、对象的共享3.1.可见性 volatile变量是Java语言提供的一种稍弱的同步机制，用来确保将变量的更新操作通知到其他线程。 使用volatile变量的条件，当且仅当满足以下条件才可以使用volatile变量：1.对变量的写入操作不依赖变量的当前值，或者你可以保证只有单个线程更新变量的值；2.该变量不会与其他状态变量一起纳入不变性条件中；3.在访问变量时不需要加锁。 在并发程序中使用和共享对象时，可以使用一些实用的策略，包括：1.线程封闭。线程封闭的对象只能由一个线程拥有，对象被封闭在该线程中，并且只能由这个线程修改。2.\b只读共享。在没有额外同步的情况下，共享的只读对象可以由多个线程并发访问，但任何线程都不能修改它。共享的只读对象包括不可变对象和\b事实不可变对象。3.\b线程安全共享。线程安全的对象在其内部实现同步，因此多个\b线程可以通过对象的公有接口来进行访问而不需要进行进一步的同步。4.\b保护对象。被保护的对象只能通过持有特定的锁来访问。线程安全对象包括封装在其他线程安全对象中的对象，以及已发布的并且由某个特定的锁保护的对象。 第四章、对象的组合4.1设计线程安全的类4.1.1收集同步需求4.1.2依赖状态的操作4.1.3状态的所有权4.2实例封闭4.2.1Java监视器模式4.2.2示例：车辆追踪4.3线程安全性的委托4.3.1基于委托的车辆追踪器4.3.2独立的状态变量4.3.3当委托失效时4.3.4发布底层的状态变量4.3.5示例：发布状态的车辆追踪器4.4在现有的线程安全类中添加功能4.4.1客户端加锁机制 Java\b的ReentrantLock和synchronized两种锁定机制的对比： ReentrantLock类实现了Lock，它拥有与synchronized相同的并发性和内存语义，但是添加了类似锁投票、定时锁等候和可中断锁等候的一些特性。此外，它还提供了在激烈争用情况下更佳的性能。（换句话说，当许多线程都想访问共享资源时，JVM 可以花更少的时候来调度线程，把更多时间用在执行线程上。） ReentrantLock默认的构造函数实现的是一个不公平锁，公平锁使线程按照请求锁的顺序依次获得锁；而不公平锁则允许讨价还价，在这种情况下，线程有时可以比先请求锁的其他线程先得到锁。公平锁带来额外的性能成本，需要诸如记账和同步。 还不要抛弃synchronized 因为对于java.util.concurrent.lock中的锁定类来说，synchronized 仍然有一些优势。比如，在使用 ReentrantLock 的时候，不能忘记释放锁；在退出synchronized块时，JVM 会为您做这件事。您很容易忘记用finally块释放锁，这对程序非常有害。您的程序能够通过测试，但会在实际工作中出现死锁，那时会很难指出原因; 当 JVM 用 synchronized 管理锁定请求和释放时，JVM 在生成线程转储时能够包括锁定信息。这些对调试非常有价值，因为它们能标识死锁或者其他异常行为的来源。Lock类只是普通的类，JVM 不知道具体哪个线程拥有Lock对象。而且，几乎每个开发人员都熟悉 synchronized，它可以在 JVM 的所有版本中工作。 4.4.2组合4.5将同步策略文档化 第五章、基础构建模块","tags":[{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"},{"name":"并发编程","slug":"并发编程","permalink":"http://yoursite.com/tags/并发编程/"}]}]